"""
Gestionnaires d'Ã©vÃ©nements pour l'interface Gradio.

Ce module contient les fonctions de callback pour les interactions
utilisateur avec l'interface web. Inclut la gestion optimisÃ©e de la mÃ©moire
pour les modÃ¨les Voxtral.
"""

import gradio as gr
import tempfile
import os

# Variables globales pour stocker les segments de rÃ©fÃ©rence actuels et les renommages
current_reference_segments = []
current_speaker_renames = {}  # Dict: SPEAKER_XX -> Nom_personnalisÃ©
current_selected_speaker = None
current_rttm_output = ""  # RTTM original pour les modifications
current_diarization_context = ""  # Diarisation formatÃ©e pour le contexte de rÃ©sumÃ©
from ..core import (
    process_file_direct_voxtral,
    on_audio_instruct_summary,
    on_audio_direct_analysis_api,
    on_audio_instruct_summary_mlx
)
from ..ai import PyAnnoteDiarizer


def build_model_name(base_model, precision):
    """
    Construit le nom complet du modÃ¨le Ã  partir du modÃ¨le de base et de la prÃ©cision.
    
    Args:
        base_model (str): ModÃ¨le de base comme "Voxtral-Mini-3B-2507"
        precision (str): PrÃ©cision comme "Default", "8bit", "4bit"
        
    Returns:
        str: Nom du modÃ¨le rÃ©el Ã  utiliser
    """
    # Mapping des modÃ¨les avec quantification
    model_mappings = {
        # Voxtral Mini
        ("Voxtral-Mini-3B-2507", "Default"): "mistralai/Voxtral-Mini-3B-2507",
        ("Voxtral-Mini-3B-2507", "8bit"): "mzbac/voxtral-mini-3b-8bit",
        ("Voxtral-Mini-3B-2507", "4bit"): "mzbac/voxtral-mini-3b-4bit-mixed",
        
        # Voxtral Small
        ("Voxtral-Small-24B-2507", "Default"): "mistralai/Voxtral-Small-24B-2507",
        ("Voxtral-Small-24B-2507", "8bit"): "VincentGOURBIN/voxtral-small-8bit",
        ("Voxtral-Small-24B-2507", "4bit"): "VincentGOURBIN/voxtral-small-4bit-mixed",
    }
    
    model_key = (base_model, precision)
    return model_mappings.get(model_key, "mistralai/Voxtral-Mini-3B-2507")





def handle_direct_transcription(
    file,
    hf_token, 
    language,
    transcription_mode,
    model_key,
    selected_sections,
    reference_speakers_data=None,
    start_trim=0,
    end_trim=0,
    chunk_duration_minutes=15,
    progress=gr.Progress()
):
    """
    GÃ¨re la transcription directe avec sections personnalisables.
    
    Args:
        file: Fichier uploadÃ©
        hf_token (str): Token HF
        language (str): Langue
        transcription_mode (str): Mode de transcription
        model_key (str): Nom du modÃ¨le ou clÃ© API Mistral
        selected_sections (list): Liste des sections Ã  inclure dans le rÃ©sumÃ©
        reference_speakers_data: Contexte de diarisation (optionnel)
        start_trim (float): Secondes Ã  enlever au dÃ©but
        end_trim (float): Secondes Ã  enlever Ã  la fin
        chunk_duration_minutes (int): DurÃ©e des chunks en minutes
    
    Returns:
        tuple: (transcription, final_summary)
    """
    try:
        # Valider et normaliser les paramÃ¨tres de trim
        start_trim = 0 if start_trim is None or start_trim == "" else float(start_trim)
        end_trim = 0 if end_trim is None or end_trim == "" else float(end_trim)
        
        # Analyser le mode de transcription pour extraire le modÃ¨le
        is_api_mode = "API" in transcription_mode
        is_mlx_mode = "MLX" in transcription_mode
        is_rocm_mode = "ROCm" in transcription_mode
        
        # Extraire le modÃ¨le du mode de transcription
        if is_api_mode:
            # Format: "API (voxtral-mini-latest)" ou "API (voxtral-small-latest)"
            model_name = transcription_mode.replace("API (", "").replace(")", "")
        elif is_mlx_mode:
            # Format: "MLX (Voxtral-Mini-3B-2507 (Default))" etc.
            # Extraire modÃ¨le de base et prÃ©cision
            start_idx = transcription_mode.find("(") + 1
            end_idx = transcription_mode.rfind(")")
            model_and_precision = transcription_mode[start_idx:end_idx]
            
            # Parser "Voxtral-Mini-3B-2507 (Default)" -> base_model, precision
            precision_start = model_and_precision.find(" (") + 2
            precision_end = model_and_precision.rfind(")")
            base_model = model_and_precision[:model_and_precision.find(" (")]
            precision = model_and_precision[precision_start:precision_end]
            
            model_name = build_model_name(base_model, precision)
        else:
            # Format: "Local (Voxtral-Mini-3B-2507 (Default))" etc.
            # Extraire modÃ¨le de base et prÃ©cision
            start_idx = transcription_mode.find("(") + 1
            end_idx = transcription_mode.rfind(")")
            model_and_precision = transcription_mode[start_idx:end_idx]
            
            # Parser "Voxtral-Mini-3B-2507 (Default)" -> base_model, precision
            precision_start = model_and_precision.find(" (") + 2
            precision_end = model_and_precision.rfind(")")
            base_model = model_and_precision[:model_and_precision.find(" (")]
            precision = model_and_precision[precision_start:precision_end]
            
            model_name = build_model_name(base_model, precision)
        
        # Traitement du fichier avec les paramÃ¨tres de trim
        wav_path = process_file_direct_voxtral(file, hf_token, start_trim, end_trim)
        
        if not wav_path:
            return "", ""
        
        # Setup progress callback
        def progress_callback(progress_ratio, message):
            progress(progress_ratio, desc=message)
        
        
        if is_api_mode:
            # Pour le mode API, model_key contient la clÃ© API
            mistral_api_key = model_key
            
            # VÃ©rifier que la clÃ© API est fournie
            if not mistral_api_key or not mistral_api_key.strip():
                return "âŒ ClÃ© API Mistral requise pour le mode API.", ""
            
            # Mode API Mistral - analyse directe avec modÃ¨le choisi
            results = on_audio_direct_analysis_api(
                wav_path=wav_path,
                mistral_api_key=mistral_api_key,
                model_name=model_name,
                language=language,
                selected_sections=selected_sections,
                chunk_duration_minutes=chunk_duration_minutes,
                reference_speakers_data=reference_speakers_data,
                progress_callback=progress_callback
            )
        elif is_mlx_mode:
            # Mode MLX - analyse directe avec MLX
            results = on_audio_instruct_summary_mlx(
                file=wav_path,
                model_name=model_name,
                language=language,
                selected_sections=selected_sections,
                chunk_duration_minutes=chunk_duration_minutes,
                reference_speakers_data=reference_speakers_data,
                start_trim=0,  # DÃ©jÃ  fait dans process_file_direct_voxtral
                end_trim=0,
                progress_callback=progress_callback
            )
        elif is_rocm_mode:
            # Mode ROCm - analyse directe avec AMD GPU
            # Extraire modÃ¨le de base et prÃ©cision (mÃªme format que Local/MLX)
            start_idx = transcription_mode.find("(") + 1
            end_idx = transcription_mode.rfind(")")
            model_and_precision = transcription_mode[start_idx:end_idx]
            
            precision_start = model_and_precision.find(" (") + 2
            precision_end = model_and_precision.rfind(")")
            base_model = model_and_precision[:model_and_precision.find(" (")]
            precision = model_and_precision[precision_start:precision_end]
            
            model_name = build_model_name(base_model, precision)
            
            results = on_audio_instruct_summary(
                file=wav_path,
                hf_token=hf_token,
                model_name=model_name,
                language=language,
                selected_sections=selected_sections,
                start_trim=0,
                end_trim=0,
                chunk_duration_minutes=chunk_duration_minutes,
                reference_speakers_data=reference_speakers_data,
                progress_callback=progress_callback,
                device_type="rocm"
            )
        else:
            # Mode analyse directe local par chunks avec modÃ¨le choisi
            results = on_audio_instruct_summary(
                file=wav_path,
                hf_token=hf_token,
                model_name=model_name,
                language=language,
                selected_sections=selected_sections,
                start_trim=0,  # DÃ©jÃ  fait dans process_file_direct_voxtral
                end_trim=0,
                chunk_duration_minutes=chunk_duration_minutes,
                reference_speakers_data=reference_speakers_data,
                progress_callback=progress_callback
            )
        
        analysis_summary = results.get("transcription", "")
        
        # En mode analyse directe, le rÃ©sumÃ© structurÃ© est dÃ©jÃ  complet
        # Pas besoin de gÃ©nÃ©ration supplÃ©mentaire de rÃ©sumÃ© final
        return "", analysis_summary  # Transcription vide, rÃ©sumÃ© structurÃ© dans final_summary
        
    finally:
        # LibÃ©ration complÃ¨te de la mÃ©moire aprÃ¨s transcription directe
        from ..ai import MemoryManager
        MemoryManager.full_cleanup()


def handle_input_mode_change(mode):
    """
    GÃ¨re le changement entre mode Audio et VidÃ©o.
    
    Args:
        mode (str): Mode sÃ©lectionnÃ© ("ğŸµ Audio" ou "ğŸ¬ VidÃ©o")
    
    Returns:
        tuple: VisibilitÃ© des sections audio et vidÃ©o
    """
    is_audio_mode = mode == "ğŸµ Audio"
    return gr.update(visible=is_audio_mode), gr.update(visible=not is_audio_mode)


def extract_audio_from_video(video_file, language):
    """
    Extrait l'audio d'un fichier vidÃ©o et bascule en mode audio.
    
    Args:
        video_file: Fichier vidÃ©o uploadÃ©
        language (str): Langue
    
    Returns:
        tuple: Chemin audio extrait, sections mises Ã  jour, paramÃ¨tres transfÃ©rÃ©s
    """
    if not video_file:
        return None, gr.update(visible=True), gr.update(visible=False), "ğŸµ Audio", None
    
    try:
        from ..audio import WavConverter
        
        # Convertir la vidÃ©o en audio
        converter = WavConverter()
        # GÃ©rer le cas oÃ¹ video_file est un chemin ou un objet fichier
        file_path = video_file if isinstance(video_file, str) else video_file.name
        audio_path = converter.convert_to_wav(file_path)
        
        # Basculer en mode audio avec les paramÃ¨tres transfÃ©rÃ©s
        return (
            audio_path,                    # audio_input
            gr.update(visible=True),       # audio_section
            gr.update(visible=False),      # video_section  
            "ğŸµ Audio",                    # input_mode
            language                       # language_audio
        )
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'extraction audio : {e}")
        return None, gr.update(), gr.update(), None, None


def handle_diarization(audio_file, hf_token, num_speakers, start_trim=0, end_trim=0):
    """
    GÃ¨re la diarisation d'un fichier audio avec pyannote.
    
    Args:
        audio_file: Fichier audio (chemin ou objet Gradio)
        hf_token (str): Token Hugging Face
        num_speakers (int, optional): Nombre de locuteurs attendu
        start_trim (float): Secondes Ã  enlever au dÃ©but
        end_trim (float): Secondes Ã  enlever Ã  la fin
        
    Returns:
        tuple: (rÃ©sultat_rttm, boutons_dynamiques, audio_player_vide)
    """
    # Valider et normaliser les paramÃ¨tres de trim
    start_trim = 0 if start_trim is None or start_trim == "" else float(start_trim)
    end_trim = 0 if end_trim is None or end_trim == "" else float(end_trim)
    
    if not audio_file:
        return "âŒ Aucun fichier audio fourni.", gr.update(choices=[]), None
    
    if not hf_token:
        return "âŒ Token Hugging Face requis pour la diarisation.", gr.update(choices=[]), None
    
    diarizer = None
    try:
        # GÃ©rer le cas oÃ¹ audio_file est un chemin ou un objet fichier
        file_path = audio_file if isinstance(audio_file, str) else audio_file.name
        
        print(f"ğŸ¤ DÃ©marrage de la diarisation: {file_path}")
        
        # Appliquer les trims comme pour Voxtral
        processed_audio_path = process_file_direct_voxtral(file_path, hf_token, start_trim, end_trim)
        if not processed_audio_path:
            return "âŒ Erreur lors du traitement du fichier audio.", gr.update(choices=[]), None
        
        print(f"ğŸµ Audio traitÃ© pour diarisation: {processed_audio_path}")
        
        # Initialiser le diariseur
        diarizer = PyAnnoteDiarizer(hf_token)
        
        # Effectuer la diarisation sur le fichier traitÃ©
        result = diarizer.diarize_audio(
            audio_path=processed_audio_path,
            num_speakers=num_speakers if num_speakers else None
        )
        
        # VÃ©rifier si on a un tuple (succÃ¨s) ou un string (erreur)
        if isinstance(result, tuple) and len(result) == 2:
            rttm_result, reference_segments = result
            
            # CrÃ©er les boutons dynamiques pour les segments de rÃ©fÃ©rence
            speaker_choices = []
            for segment in reference_segments:
                speaker_label = f"ğŸ¤ {segment['speaker']} ({segment['duration']:.1f}s)"
                speaker_choices.append(speaker_label)
            
            print(f"ğŸ“‹ Boutons crÃ©Ã©s: {len(speaker_choices)} locuteurs")
            
            # Stocker les segments et le RTTM pour l'usage ultÃ©rieur
            global current_reference_segments, current_rttm_output, current_speaker_renames, current_diarization_context
            current_reference_segments = reference_segments
            current_rttm_output = rttm_result
            current_speaker_renames = {}  # Reset des renommages
            current_diarization_context = convert_rttm_to_tagged_format(rttm_result)  # Format avec balises pour le contexte
            
            # Afficher la section de renommage si on a des locuteurs
            rename_section_visible = len(speaker_choices) > 0
            
            return (
                gr.update(choices=speaker_choices, visible=len(speaker_choices) > 0), 
                None,
                gr.update(visible=rename_section_visible)
            )
        else:
            # Cas d'erreur - result est un string d'erreur
            print(f"âŒ Erreur diarisation: {result}")
            return gr.update(choices=[]), None, gr.update(visible=False)
        
    except Exception as e:
        print(f"âŒ Erreur lors de la diarisation: {e}")
        return f"âŒ Erreur lors de la diarisation: {str(e)}", gr.update(choices=[]), None, gr.update(visible=False)
    
    finally:
        # LibÃ©rer les ressources
        if diarizer is not None:
            diarizer.cleanup()


def handle_speaker_selection(speaker_button, current_name_in_field):
    """
    GÃ¨re la sÃ©lection d'un locuteur pour Ã©couter son segment de rÃ©fÃ©rence.
    Sauvegarde automatiquement le nom du locuteur prÃ©cÃ©dent.
    
    Args:
        speaker_button (str): Label du bouton sÃ©lectionnÃ©
        current_name_in_field (str): Nom actuellement dans le champ de saisie
        
    Returns:
        tuple: (chemin_audio, nom_actuel_du_locuteur)
    """
    global current_reference_segments, current_selected_speaker, current_speaker_renames
    
    # Sauvegarder le nom du locuteur prÃ©cÃ©dent s'il y en avait un
    if current_selected_speaker and current_name_in_field and current_name_in_field.strip():
        old_name = current_name_in_field.strip()
        if old_name != current_selected_speaker:  # Seulement si diffÃ©rent de l'ID original
            current_speaker_renames[current_selected_speaker] = old_name
            print(f"ğŸ’¾ Sauvegarde automatique: {current_selected_speaker} â†’ {old_name}")
    
    if not speaker_button or not current_reference_segments:
        return None, ""
    
    try:
        # Extraire le nom du locuteur du label du bouton (format: "ğŸ¤ SPEAKER_XX (5.2s)")
        speaker_name = speaker_button.split(" ")[1]  # RÃ©cupÃ¨re "SPEAKER_XX"
        current_selected_speaker = speaker_name
        
        print(f"ğŸ” Recherche du segment pour: {speaker_name}")
        
        # Chercher le segment correspondant
        for segment in current_reference_segments:
            if segment['speaker'] == speaker_name:
                print(f"âœ… Segment trouvÃ©: {segment['audio_path']}")
                
                # Retourner le nom actuel (renommÃ© ou original)
                current_name = current_speaker_renames.get(speaker_name, speaker_name)
                return segment['audio_path'], current_name
        
        print(f"âŒ Aucun segment trouvÃ© pour: {speaker_name}")
        return None, ""
        
    except Exception as e:
        print(f"âŒ Erreur lors de la sÃ©lection du locuteur: {e}")
        return None, ""


def handle_speaker_rename(current_name_in_field):
    """
    Applique tous les renommages Ã  la diarisation RTTM complÃ¨te.
    
    Args:
        current_name_in_field (str): Nom dans le champ (pour sauvegarder le dernier)
        
    Returns:
        tuple: (rttm_modifiÃ©, zone_visible)
    """
    global current_selected_speaker, current_speaker_renames, current_rttm_output, current_diarization_context
    
    # Sauvegarder le nom du locuteur actuellement sÃ©lectionnÃ©
    if current_selected_speaker and current_name_in_field and current_name_in_field.strip():
        clean_name = current_name_in_field.strip()
        if clean_name != current_selected_speaker:  # Seulement si diffÃ©rent de l'ID original
            current_speaker_renames[current_selected_speaker] = clean_name
            print(f"ğŸ’¾ Sauvegarde finale: {current_selected_speaker} â†’ {clean_name}")
    
    if not current_rttm_output or not current_speaker_renames:
        return "", gr.update(visible=False)
    
    # Mettre Ã  jour le contexte de diarisation avec les nouveaux noms (format avec balises)
    current_diarization_context = apply_renames_to_rttm(current_rttm_output, current_speaker_renames)
    
    # GÃ©nÃ©rer le rÃ©sumÃ© lisible des locuteurs pour l'affichage utilisateur
    speakers_summary = generate_speakers_summary(current_rttm_output, current_speaker_renames)
    
    print(f"âœ… Locuteurs renommÃ©s: {len(current_speaker_renames)} modifications")
    print(f"ğŸ“‹ Contexte de diarisation mis Ã  jour pour les futurs rÃ©sumÃ©s")
    
    return speakers_summary, gr.update(visible=True)


def apply_renames_to_rttm(original_rttm: str, renames: dict) -> str:
    """
    Applique les renommages de locuteurs au RTTM complet.
    
    Args:
        original_rttm (str): RTTM original
        renames (dict): Dict des renommages SPEAKER_XX -> Nom
        
    Returns:
        str: RTTM modifiÃ© avec les nouveaux noms
    """
    if not original_rttm or not renames:
        return original_rttm
    
    modified_lines = []
    
    for line in original_rttm.split('\n'):
        if line.strip() and line.startswith('SPEAKER'):
            # Parser la ligne RTTM: SPEAKER file 1 start duration <NA> <NA> speaker_id <NA> <NA>
            parts = line.split()
            if len(parts) >= 8:
                speaker_id = parts[7]  # L'ID du locuteur est Ã  l'index 7
                
                # Remplacer par le nom personnalisÃ© si disponible
                if speaker_id in renames:
                    custom_name = renames[speaker_id]
                    start_time = parts[3]
                    duration = parts[4]
                    
                    # Format avec balises: "<locuteur>Jean</locuteur> <dÃ©but>358.270</dÃ©but> <fin>360.666</fin>"
                    end_time = float(start_time) + float(duration)
                    modified_line = f"<locuteur>{custom_name}</locuteur> <dÃ©but>{start_time}</dÃ©but> <fin>{end_time:.3f}</fin>"
                    modified_lines.append(modified_line)
                else:
                    # Format avec balises mÃªme sans renommage
                    start_time = parts[3]
                    duration = parts[4]
                    end_time = float(start_time) + float(duration)
                    modified_line = f"<locuteur>{speaker_id}</locuteur> <dÃ©but>{start_time}</dÃ©but> <fin>{end_time:.3f}</fin>"
                    modified_lines.append(modified_line)
            else:
                # Ligne malformÃ©e, garder telle quelle
                modified_lines.append(line)
        else:
            # Ligne non-SPEAKER, garder telle quelle
            modified_lines.append(line)
    
    return '\n'.join(modified_lines)


def convert_rttm_to_tagged_format(rttm_output: str) -> str:
    """
    Convertit un RTTM brut au format avec balises pour le contexte de diarisation.
    
    Args:
        rttm_output (str): RTTM brut de pyannote
        
    Returns:
        str: RTTM formatÃ© avec balises
    """
    if not rttm_output:
        return ""
    
    tagged_lines = []
    
    for line in rttm_output.split('\n'):
        if line.strip() and line.startswith('SPEAKER'):
            # Parser la ligne RTTM: SPEAKER file 1 start duration <NA> <NA> speaker_id <NA> <NA>
            parts = line.split()
            if len(parts) >= 8:
                speaker_id = parts[7]  # L'ID du locuteur est Ã  l'index 7
                start_time = parts[3]
                duration = parts[4]
                end_time = float(start_time) + float(duration)
                
                # Format avec balises
                tagged_line = f"<locuteur>{speaker_id}</locuteur> <dÃ©but>{start_time}</dÃ©but> <fin>{end_time:.3f}</fin>"
                tagged_lines.append(tagged_line)
    
    return '\n'.join(tagged_lines)


def generate_speakers_summary(original_rttm: str, renames: dict) -> str:
    """
    GÃ©nÃ¨re un rÃ©sumÃ© lisible des locuteurs identifiÃ©s.
    
    Args:
        original_rttm (str): RTTM original
        renames (dict): Dict des renommages SPEAKER_XX -> Nom
        
    Returns:
        str: RÃ©sumÃ© lisible des locuteurs
    """
    if not original_rttm:
        return ""
    
    # Compter les segments par locuteur
    speaker_segments = {}
    total_duration_by_speaker = {}
    
    for line in original_rttm.split('\n'):
        if line.strip() and line.startswith('SPEAKER'):
            parts = line.split()
            if len(parts) >= 8:
                speaker_id = parts[7]
                duration = float(parts[4])
                
                if speaker_id not in speaker_segments:
                    speaker_segments[speaker_id] = 0
                    total_duration_by_speaker[speaker_id] = 0.0
                
                speaker_segments[speaker_id] += 1
                total_duration_by_speaker[speaker_id] += duration
    
    # GÃ©nÃ©rer le rÃ©sumÃ©
    summary_lines = []
    for speaker_id in sorted(speaker_segments.keys()):
        display_name = renames.get(speaker_id, speaker_id)
        segment_count = speaker_segments[speaker_id]
        total_time = total_duration_by_speaker[speaker_id]
        
        # Formatage du temps (minutes:secondes)
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        time_str = f"{minutes}:{seconds:02d}"
        
        summary_lines.append(f"ğŸ‘¤ {display_name}: {segment_count} segments, {time_str} de parole")
    
    if summary_lines:
        return "\n".join(summary_lines)
    else:
        return "Aucun locuteur identifiÃ©"




